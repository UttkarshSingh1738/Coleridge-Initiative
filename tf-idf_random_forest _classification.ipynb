{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":25925,"databundleVersionId":2089362,"sourceType":"competition"}],"dockerImageVersionId":30066,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\nimport os\nimport re\nimport json\nimport glob\nfrom collections import defaultdict\nfrom textblob import TextBlob\nfrom functools import partial\nimport pandas as pd\nfrom tqdm.autonotebook import tqdm\nimport os\nimport matplotlib.pyplot as plt\nimport json\nimport nltk\nimport re\nimport concurrent.futures\nimport numpy as np\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom collections import OrderedDict, Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split \nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\n\nimport nltk\nimport spacy\nnlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\nnlp.max_length = 4000000\nfrom nltk.probability import FreqDist\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom tqdm.autonotebook import tqdm\nimport string\n\n%matplotlib inline\n\nos.listdir('/kaggle/input/coleridgeinitiative-show-us-the-data/')","metadata":{"_uuid":"d0ebc002-659a-4d2d-9a1c-e347a72da086","_cell_guid":"d0d6d5f0-fbb8-4523-b627-32cfdd0ffacc","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":false,"execution":{"iopub.status.busy":"2024-07-25T11:07:50.332994Z","iopub.execute_input":"2024-07-25T11:07:50.333673Z","iopub.status.idle":"2024-07-25T11:08:03.310630Z","shell.execute_reply.started":"2024-07-25T11:07:50.333531Z","shell.execute_reply":"2024-07-25T11:08:03.309584Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"['sample_submission.csv', 'train.csv', 'test', 'train']"},"metadata":{}}]},{"cell_type":"code","source":"# reading csv files and train & test file paths\ntrain_df = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/train.csv')\nsample_sub = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\ntrain_files_path = '../input/coleridgeinitiative-show-us-the-data/train'\ntest_files_path = '../input/coleridgeinitiative-show-us-the-data/test'","metadata":{"_uuid":"3ed6d675-f834-4684-9978-a10d639fce40","_cell_guid":"a95311ea-221e-40f7-a7a5-d13ede766e17","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-07-25T11:09:35.732622Z","iopub.execute_input":"2024-07-25T11:09:35.733056Z","iopub.status.idle":"2024-07-25T11:09:35.885099Z","shell.execute_reply.started":"2024-07-25T11:09:35.733020Z","shell.execute_reply":"2024-07-25T11:09:35.883967Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_df = train_df.sample(frac=1).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T11:09:40.939389Z","iopub.execute_input":"2024-07-25T11:09:40.939808Z","iopub.status.idle":"2024-07-25T11:09:40.950403Z","shell.execute_reply.started":"2024-07-25T11:09:40.939773Z","shell.execute_reply":"2024-07-25T11:09:40.949250Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def read_append_return(filename, train_files_path=train_files_path, output='text'):\n    \"\"\"\n    Function to read json file and then return the text data from them and append to the dataframe\n    \"\"\"\n    json_path = os.path.join(train_files_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data","metadata":{"_uuid":"7ed7e118-7804-44a6-a06d-6aa80bf7e447","_cell_guid":"797505f5-4fa5-4fb4-849c-02f24de9682f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-07-25T11:09:44.234642Z","iopub.execute_input":"2024-07-25T11:09:44.235013Z","iopub.status.idle":"2024-07-25T11:09:44.242499Z","shell.execute_reply.started":"2024-07-25T11:09:44.234977Z","shell.execute_reply":"2024-07-25T11:09:44.241578Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"%%time\ntqdm.pandas()   #tqdm is used to show any code running with a progress bar. \ntrain_df['text'] = train_df['Id'].progress_apply(read_append_return)","metadata":{"_uuid":"6360ee31-acbb-44df-bbc2-d6a2f5774597","_cell_guid":"0a169c53-f2f4-4262-9f49-bfbc85f3817d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-07-25T11:09:47.812385Z","iopub.execute_input":"2024-07-25T11:09:47.812762Z","iopub.status.idle":"2024-07-25T11:10:54.721491Z","shell.execute_reply.started":"2024-07-25T11:09:47.812726Z","shell.execute_reply":"2024-07-25T11:10:54.720427Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/19661 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a7cdc26f6c84dda9847bc74a9e23d95"}},"metadata":{}},{"name":"stdout","text":"CPU times: user 11.2 s, sys: 5.21 s, total: 16.5 s\nWall time: 1min 6s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\ntqdm.pandas()\nsample_sub['text'] = sample_sub['Id'].progress_apply(partial(read_append_return, train_files_path=test_files_path))","metadata":{"execution":{"iopub.status.busy":"2024-07-25T11:10:57.464256Z","iopub.execute_input":"2024-07-25T11:10:57.464656Z","iopub.status.idle":"2024-07-25T11:10:57.525878Z","shell.execute_reply.started":"2024-07-25T11:10:57.464619Z","shell.execute_reply":"2024-07-25T11:10:57.524983Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"470090dcd18441e6a8581c78b7b1da30"}},"metadata":{}},{"name":"stdout","text":"CPU times: user 41.4 ms, sys: 3.67 ms, total: 45.1 ms\nWall time: 55.9 ms\n","output_type":"stream"}]},{"cell_type":"code","source":"def text_cleaning(text):\n    '''\n    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n    text - Sentence that needs to be cleaned\n    '''\n    text = ''.join([k for k in text if k not in string.punctuation])\n    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n#     text = re.sub(\"/'+/g\", ' ', text)\n    \n    return text","metadata":{"_uuid":"32e8df88-a0e1-4b25-a441-6c81a724b8fa","_cell_guid":"b4866554-9a38-46d5-86d1-841efddd9919","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-07-25T11:11:00.233710Z","iopub.execute_input":"2024-07-25T11:11:00.234079Z","iopub.status.idle":"2024-07-25T11:11:00.239448Z","shell.execute_reply.started":"2024-07-25T11:11:00.234046Z","shell.execute_reply":"2024-07-25T11:11:00.238624Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"%%time\ntqdm.pandas()\ntrain_df['text'] = train_df['text'].progress_apply(text_cleaning)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T11:11:02.176164Z","iopub.execute_input":"2024-07-25T11:11:02.176630Z","iopub.status.idle":"2024-07-25T11:14:44.656118Z","shell.execute_reply.started":"2024-07-25T11:11:02.176585Z","shell.execute_reply":"2024-07-25T11:14:44.655234Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/19661 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f090ee6b4b614c23be37f972fac019fb"}},"metadata":{}},{"name":"stdout","text":"CPU times: user 3min 38s, sys: 5.84 s, total: 3min 43s\nWall time: 3min 42s\n","output_type":"stream"}]},{"cell_type":"code","source":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()","metadata":{"_uuid":"e4627869-d71b-4514-bea0-4f9cd1da1a8d","_cell_guid":"3aeb8c4f-b38e-4ea1-8b9d-ff7bbe43bba1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-07-25T11:14:44.658795Z","iopub.execute_input":"2024-07-25T11:14:44.659086Z","iopub.status.idle":"2024-07-25T11:14:44.663860Z","shell.execute_reply.started":"2024-07-25T11:14:44.659057Z","shell.execute_reply":"2024-07-25T11:14:44.662617Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_df.head(4)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T11:14:44.665700Z","iopub.execute_input":"2024-07-25T11:14:44.666081Z","iopub.status.idle":"2024-07-25T11:14:44.691155Z","shell.execute_reply.started":"2024-07-25T11:14:44.666038Z","shell.execute_reply":"2024-07-25T11:14:44.690172Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                     Id  \\\n0  8d61b2de-3b1e-4232-863b-9ba342d9ad65   \n1  cf0223e8-f25e-4ea4-b63e-e971d72a9caa   \n2  85b70b11-8895-456b-b133-df89b0bbff8a   \n3  b21c09ca-2eeb-499c-9e73-9103871abafd   \n\n                                           pub_title  \\\n0  Public School Choice and Racial Sorting: An Ex...   \n1  Determining the Effects of Pre-College STEM Co...   \n2  Data Augmentation Using Learned Transformation...   \n3  Vestibular Loss in Older Adults Is Associated ...   \n\n                                       dataset_title  \\\n0                                Common Core of Data   \n1                       Education Longitudinal Study   \n2  Alzheimer's Disease Neuroimaging Initiative (A...   \n3       Baltimore Longitudinal Study of Aging (BLSA)   \n\n                           dataset_label  \\\n0               NCES Common Core of Data   \n1           Education Longitudinal Study   \n2                                   ADNI   \n3  Baltimore Longitudinal Study of Aging   \n\n                           cleaned_label  \\\n0               nces common core of data   \n1           education longitudinal study   \n2                                   adni   \n3  baltimore longitudinal study of aging   \n\n                                                text  \n0  there has been a longstanding concern among ed...  \n1  many stem studies have focused on traditional ...  \n2  image segmentation is an important task in man...  \n3  background vestibular inputs have been shown t...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>pub_title</th>\n      <th>dataset_title</th>\n      <th>dataset_label</th>\n      <th>cleaned_label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8d61b2de-3b1e-4232-863b-9ba342d9ad65</td>\n      <td>Public School Choice and Racial Sorting: An Ex...</td>\n      <td>Common Core of Data</td>\n      <td>NCES Common Core of Data</td>\n      <td>nces common core of data</td>\n      <td>there has been a longstanding concern among ed...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cf0223e8-f25e-4ea4-b63e-e971d72a9caa</td>\n      <td>Determining the Effects of Pre-College STEM Co...</td>\n      <td>Education Longitudinal Study</td>\n      <td>Education Longitudinal Study</td>\n      <td>education longitudinal study</td>\n      <td>many stem studies have focused on traditional ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>85b70b11-8895-456b-b133-df89b0bbff8a</td>\n      <td>Data Augmentation Using Learned Transformation...</td>\n      <td>Alzheimer's Disease Neuroimaging Initiative (A...</td>\n      <td>ADNI</td>\n      <td>adni</td>\n      <td>image segmentation is an important task in man...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b21c09ca-2eeb-499c-9e73-9103871abafd</td>\n      <td>Vestibular Loss in Older Adults Is Associated ...</td>\n      <td>Baltimore Longitudinal Study of Aging (BLSA)</td>\n      <td>Baltimore Longitudinal Study of Aging</td>\n      <td>baltimore longitudinal study of aging</td>\n      <td>background vestibular inputs have been shown t...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sample_sub","metadata":{"execution":{"iopub.status.busy":"2024-07-25T11:14:44.692763Z","iopub.execute_input":"2024-07-25T11:14:44.693152Z","iopub.status.idle":"2024-07-25T11:14:44.706807Z","shell.execute_reply.started":"2024-07-25T11:14:44.693109Z","shell.execute_reply":"2024-07-25T11:14:44.705983Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                                     Id  PredictionString  \\\n0  2100032a-7c33-4bff-97ef-690822c43466               NaN   \n1  2f392438-e215-4169-bebf-21ac4ff253e1               NaN   \n2  3f316b38-1a24-45a9-8d8c-4e05a42257c6               NaN   \n3  8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60               NaN   \n\n                                                text  \n0  Cognitive deficits and reduced educational ach...  \n1  This report describes how the education system...  \n2  Cape Hatteras National Seashore (CAHA), locate...  \n3  A significant body of research has been conduc...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>PredictionString</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n      <td>NaN</td>\n      <td>Cognitive deficits and reduced educational ach...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2f392438-e215-4169-bebf-21ac4ff253e1</td>\n      <td>NaN</td>\n      <td>This report describes how the education system...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3f316b38-1a24-45a9-8d8c-4e05a42257c6</td>\n      <td>NaN</td>\n      <td>Cape Hatteras National Seashore (CAHA), locate...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60</td>\n      <td>NaN</td>\n      <td>A significant body of research has been conduc...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"Train_df = train_df.copy()","metadata":{"execution":{"iopub.status.busy":"2024-07-25T11:14:52.078443Z","iopub.execute_input":"2024-07-25T11:14:52.078840Z","iopub.status.idle":"2024-07-25T11:14:52.086876Z","shell.execute_reply.started":"2024-07-25T11:14:52.078808Z","shell.execute_reply":"2024-07-25T11:14:52.085688Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"Train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-25T11:14:57.774745Z","iopub.execute_input":"2024-07-25T11:14:57.775102Z","iopub.status.idle":"2024-07-25T11:14:57.791798Z","shell.execute_reply.started":"2024-07-25T11:14:57.775071Z","shell.execute_reply":"2024-07-25T11:14:57.790547Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                                     Id  \\\n0  8d61b2de-3b1e-4232-863b-9ba342d9ad65   \n1  cf0223e8-f25e-4ea4-b63e-e971d72a9caa   \n2  85b70b11-8895-456b-b133-df89b0bbff8a   \n3  b21c09ca-2eeb-499c-9e73-9103871abafd   \n4  64488cc2-7515-4385-a3b7-64c50767d526   \n\n                                           pub_title  \\\n0  Public School Choice and Racial Sorting: An Ex...   \n1  Determining the Effects of Pre-College STEM Co...   \n2  Data Augmentation Using Learned Transformation...   \n3  Vestibular Loss in Older Adults Is Associated ...   \n4  Prediction of Incipient Alzheimer's Disease De...   \n\n                                       dataset_title  \\\n0                                Common Core of Data   \n1                       Education Longitudinal Study   \n2  Alzheimer's Disease Neuroimaging Initiative (A...   \n3       Baltimore Longitudinal Study of Aging (BLSA)   \n4  Alzheimer's Disease Neuroimaging Initiative (A...   \n\n                           dataset_label  \\\n0               NCES Common Core of Data   \n1           Education Longitudinal Study   \n2                                   ADNI   \n3  Baltimore Longitudinal Study of Aging   \n4                                   ADNI   \n\n                           cleaned_label  \\\n0               nces common core of data   \n1           education longitudinal study   \n2                                   adni   \n3  baltimore longitudinal study of aging   \n4                                   adni   \n\n                                                text  \n0  there has been a longstanding concern among ed...  \n1  many stem studies have focused on traditional ...  \n2  image segmentation is an important task in man...  \n3  background vestibular inputs have been shown t...  \n4  background mild cognitive impairment mci is a ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>pub_title</th>\n      <th>dataset_title</th>\n      <th>dataset_label</th>\n      <th>cleaned_label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8d61b2de-3b1e-4232-863b-9ba342d9ad65</td>\n      <td>Public School Choice and Racial Sorting: An Ex...</td>\n      <td>Common Core of Data</td>\n      <td>NCES Common Core of Data</td>\n      <td>nces common core of data</td>\n      <td>there has been a longstanding concern among ed...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cf0223e8-f25e-4ea4-b63e-e971d72a9caa</td>\n      <td>Determining the Effects of Pre-College STEM Co...</td>\n      <td>Education Longitudinal Study</td>\n      <td>Education Longitudinal Study</td>\n      <td>education longitudinal study</td>\n      <td>many stem studies have focused on traditional ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>85b70b11-8895-456b-b133-df89b0bbff8a</td>\n      <td>Data Augmentation Using Learned Transformation...</td>\n      <td>Alzheimer's Disease Neuroimaging Initiative (A...</td>\n      <td>ADNI</td>\n      <td>adni</td>\n      <td>image segmentation is an important task in man...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b21c09ca-2eeb-499c-9e73-9103871abafd</td>\n      <td>Vestibular Loss in Older Adults Is Associated ...</td>\n      <td>Baltimore Longitudinal Study of Aging (BLSA)</td>\n      <td>Baltimore Longitudinal Study of Aging</td>\n      <td>baltimore longitudinal study of aging</td>\n      <td>background vestibular inputs have been shown t...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>64488cc2-7515-4385-a3b7-64c50767d526</td>\n      <td>Prediction of Incipient Alzheimer's Disease De...</td>\n      <td>Alzheimer's Disease Neuroimaging Initiative (A...</td>\n      <td>ADNI</td>\n      <td>adni</td>\n      <td>background mild cognitive impairment mci is a ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"text = Train_df[\"text\"].tolist()\ntesttext = sample_sub['text'].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-07-25T11:15:01.617696Z","iopub.execute_input":"2024-07-25T11:15:01.618045Z","iopub.status.idle":"2024-07-25T11:15:01.624913Z","shell.execute_reply.started":"2024-07-25T11:15:01.618015Z","shell.execute_reply":"2024-07-25T11:15:01.623538Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntext2vec = []\nvectorizer = TfidfVectorizer()\nvectorizer.fit(text)\nfor i in range(0,1):\n    vec = vectorizer.transform((text[i])) \n    text2vec.append(np.asarray(vec))","metadata":{"execution":{"iopub.status.busy":"2024-07-25T11:15:03.385538Z","iopub.execute_input":"2024-07-25T11:15:03.385927Z","iopub.status.idle":"2024-07-25T11:17:07.337451Z","shell.execute_reply.started":"2024-07-25T11:15:03.385895Z","shell.execute_reply":"2024-07-25T11:17:07.335531Z"},"trusted":true},"execution_count":16,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-8e056cd4f318>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtext2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1872\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'The TF-IDF vectorizer is not fitted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1874\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1875\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m             raise ValueError(\n\u001b[0;32m-> 1250\u001b[0;31m                 \u001b[0;34m\"Iterable over raw text documents expected, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m                 \"string object received.\")\n\u001b[1;32m   1252\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."],"ename":"ValueError","evalue":"Iterable over raw text documents expected, string object received.","output_type":"error"}]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\nvectorizer.fit(text)\ntest_text2vec  = [vectorizer.transform((testtext[i])) \n            for i in range(0,len(Train_df['text']))]","metadata":{"execution":{"iopub.status.busy":"2021-06-18T20:49:54.533837Z","iopub.execute_input":"2021-06-18T20:49:54.53442Z","iopub.status.idle":"2021-06-18T20:49:54.540217Z","shell.execute_reply.started":"2021-06-18T20:49:54.534368Z","shell.execute_reply":"2021-06-18T20:49:54.539119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sparce_matrix = text2vec\ny = Train_df.iloc[:,2].values \nX = sparce_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = LabelEncoder()\ny = encoder.fit_transform(y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decoded = encoder.inverse_transform(y)\ndecoded[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=45)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc1=RandomForestClassifier(n_estimators= 50, max_depth=15, bootstrap=True, random_state=45)\nrfc1.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\ny_pred = rfc1.predict(X_test)\nprint('Accuracy:',accuracy_score(y_test,y_pred))\n# print('Confusion matrix:\\n',confusion_matrix(y_test,y_pred))\n# print('Classification report:\\n',classification_report(y_test,y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def top_n_pred_prob_df(n, model, X_test, column_name):\n  predictions = model.predict_proba(X_test)\n  preds_idx = np.argsort(-predictions) \n  classes = pd.DataFrame(model.classes_, columns=['class_name'])\n  classes.reset_index(inplace=True)\n  top_n_preds = pd.DataFrame()\n  for i in range(n):\n    top_n_preds[column_name + 'Prediction {}_num'.format(i)] =     [preds_idx[doc][i] for doc in range(len(X_test))]\n    top_n_preds[column_name + 'Prediction {} probability'.format(i)] = [predictions[doc][preds_idx[doc][i]] for doc in range(len(X_test))]\n    top_n_preds = top_n_preds.merge(classes, how='left', left_on= column_name + 'Prediction {}_num'.format(i), right_on='index')\n    top_n_preds = top_n_preds.rename(columns={'class_name': column_name + 'Prediction {}'.format(i)})\n    try: top_n_preds.drop(columns=['index', column_name + 'Prediction {}_num'.format(i)], inplace=True) \n    except: pass\n  return top_n_preds.iloc[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Prediction_df = pd.DataFrame()\n\nfor i in range (4):\n    x = top_n_pred_prob_df(10,rfc1,np.array(test_text2vec[i]).reshape(1, -1), \"\")\n    Prediction_df = Prediction_df.append(x, ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Prediction_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicts = []\nfor i in range (4):\n    lol  = np.array(test_text2vec[i]).reshape(1, -1)\n    pred_test =  rfc1.predict(lol)\n    X = encoder.inverse_transform(pred_test)\n    X = X.tolist()[0]\n    predicts.append(X)\npredicts","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicts1 = []\npredicts2 = []\npredicts3 = []\npredicts4 = []\npredicts5 = []\n\nfor i in range (4):\n    X = int(Prediction_df.iloc[i]['Prediction 0'])\n    newArray = np.append (0, X)\n    newArray = np.delete(newArray, 0)\n    Y = encoder.inverse_transform(newArray)\n    Y = Y.tolist()[0]\n    predicts1.append(Y)\n    X = int(Prediction_df.iloc[i]['Prediction 1'])\n    newArray = np.append (0, X)\n    newArray = np.delete(newArray, 0)\n    Y = encoder.inverse_transform(newArray)\n    Y = Y.tolist()[0]\n    predicts2.append(Y)\n    X = int(Prediction_df.iloc[i]['Prediction 2'])\n    newArray = np.append (0, X)\n    newArray = np.delete(newArray, 0)\n    Y = encoder.inverse_transform(newArray)\n    Y = Y.tolist()[0]\n    predicts3.append(Y)\n    X = int(Prediction_df.iloc[i]['Prediction 3'])\n    newArray = np.append (0, X)\n    newArray = np.delete(newArray, 0)\n    Y = encoder.inverse_transform(newArray)\n    Y = Y.tolist()[0]\n    predicts4.append(Y)\n    X = int(Prediction_df.iloc[i]['Prediction 4'])\n    newArray = np.append (0, X)\n    newArray = np.delete(newArray, 0)\n    Y = encoder.inverse_transform(newArray)\n    Y = Y.tolist()[0]\n    predicts5.append(Y)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp1 = [x.lower() for x in predicts1]\ntemp2 = [x.lower() for x in predicts2]\ntemp3 = [x.lower() for x in predicts3]\ntemp4 = [x.lower() for x in predicts4]\ntemp5 = [x.lower() for x in predicts5]\n\n\nexisting_labels = (temp1 + temp2 + temp3 + temp4 + temp5)\nid_list = []\nlables_list = []\nfor index, row in tqdm(sample_sub.iterrows()):\n    sample_text = row['text']\n    row_id = row['Id']\n    temp_df = train_df[train_df['text'] == text_cleaning(sample_text)]\n    cleaned_labels = temp_df['cleaned_label'].to_list()\n    for known_label in existing_labels:\n        if known_label in sample_text.lower():\n            cleaned_labels.append(clean_text(known_label))\n    cleaned_labels = [clean_text(x) for x in cleaned_labels]\n    cleaned_labels = set(cleaned_labels)\n    lables_list.append('|'.join(cleaned_labels))\n    id_list.append(row_id)","metadata":{"_uuid":"259fcbec-4a0a-4ae6-a568-6333491e7e0e","_cell_guid":"34c61100-cb59-41a7-94e5-fc9d0cfc5c70","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_subf = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\nsample_subf.PredictionString = lables_list\nsample_subf.to_csv('./submission.csv', index=False)","metadata":{"_uuid":"838e25a1-fc62-4740-ac7b-cf7185fa179b","_cell_guid":"a150f926-56d4-478d-a1fd-83a307ac99c2","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_subf.head()","metadata":{"_uuid":"14e7c146-8746-44fe-adcf-bef10a5641d5","_cell_guid":"3f2cb753-d18c-433c-a7ad-588c29d8cfef","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}